{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efd3293-9280-42de-aafc-e271117a1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6650dcb-3bb8-4bd4-9914-d479da56cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Environment Variables\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "#langsmith_endpoint = os.getenv('LANGSMITH_ENDPOINT') \n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "#langsmith_api_key = os.getenv('LANGSMITH_API_KEY') \n",
    "\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"pr-silver-stencil-88\"\n",
    "#langsmith_project = os.getenv('LANGSMITH_PROJECT') \n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"cL9j05Qnp17FgSxqAlO4N7V17AXdtSVt\"\n",
    "\n",
    "api_key = os.getenv('MISTRAL_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e94f607-928e-403f-bc39-60808225692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ba0ef7-1abe-4e33-9b60-aef4e0771522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Joy! Nice to meet you. How are you today? Let's chat about anything you'd like. üòä\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 35, 'completion_tokens': 26}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-2ee8fae4-a477-4541-bab3-07d55126d7f8-0', usage_metadata={'input_tokens': 9, 'output_tokens': 26, 'total_tokens': 35})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Joy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bb08a3-6232-4068-af0e-458b6c1a9797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to personal information about you, including your name. If you'd like, you can tell me your name, and I can use it during our conversation. How would you like me to address you?\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 56, 'completion_tokens': 47}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-e4dfaece-489f-4760-9549-9b7f3fd15879-0', usage_metadata={'input_tokens': 9, 'output_tokens': 47, 'total_tokens': 56})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2f0f01-ea03-4e4f-b88f-b3435dfc887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You just told me your name is Joy! Is that correct?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 28, 'total_tokens': 41, 'completion_tokens': 13}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-b9e30d6c-e2f3-48bc-8e16-fb0a7b3d3e0b-0', usage_metadata={'input_tokens': 28, 'output_tokens': 13, 'total_tokens': 41})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Joy\"),\n",
    "        AIMessage(content=\"Hello Joy! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440a01fe-bc6e-486c-9f45-d5cdcd7ebbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Message persistence using LangGraph\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d461bf47-c283-45d8-9541-eaea26c1d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4081d90b-02b2-454a-8e1b-c04f490d33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Joy! Nice to meet you. How are you today? Is there something you would like to talk about or do?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Joy.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9e84d6-2649-45d5-8429-31dcdbee97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You told me your name is Joy!\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cdf9749-912a-4de5-81bd-b95fb743714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm an artificial intelligence and don't have real-time personal information about you. I can only see and respond to the text you type here. If you'd like me to use a specific name when addressing you, please tell me what it is, and I'll be happy to use it!\n"
     ]
    }
   ],
   "source": [
    "# Changing thread reference leads to fresh conversation history \n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f723fb0-f8c7-4fda-9d7d-a878ee672735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I remember you told me your name is Joy! You just asked me the same question. Do you remember? If you want me to use a different name, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Going back to the original conversation with old thread to get conversation history\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191abeb0-6022-4148-98f9-ffb295fdfb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm an artificial intelligence and don't have real-time personal information about you. I can help you with a wide range of topics, but I don't know your name. If you'd like, you can tell me your name, and I can use it in our conversation!\n"
     ]
    }
   ],
   "source": [
    "# Using Async workflow \n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49ac7a97-563a-419d-a2dd-e17eb5b6a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a199e3d-eed5-4b5a-a1cb-be836f7bb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry now the responses \n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "#Update application state as we have two inputs - messages and language\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd77c8a4-fc23-4bc4-948f-086e618df73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "‡§π‡•á‡§≤‡•ç‡§≤‡•ã! ‡§Æ‡•à‡§Ç ‡§ú‡•â‡§Ø ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à. ‡§§‡•Å‡§Æ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Joy.\"\n",
    "language = \"Hindi\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "940f8446-d93c-4e2a-b13b-073057516218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•â‡§Ø ‡§π‡•à, ‡§∏‡§π‡•Ä?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73275f19-94de-456c-abe9-07fd5c45c017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm joy\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trim messages from message history to set the context in context window and avoiding overflow.\n",
    "\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=55,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm joy\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c72bfdf-4479-496c-9843-4a5b03dd2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "# use trimmed message in prompt\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": state[\"messages\"], \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c01bcad9-5d72-45f1-8472-b665a886352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tumne poocha tha, \"2 + 2 kya hota hai?\" aur mere se jawab diya gaya tha, \"4\"."
     ]
    }
   ],
   "source": [
    "# Streaming in response to make chat more interactive\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"Which math query I asked?\"\n",
    "language = \"Hindi\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode = \"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54c611cd-725b-4a08-8154-4b7378b5e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tum vanilla ice cream pasand karate ho.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"Which icecream i like?\"\n",
    "language = \"Hindi\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6dadae-9d7f-4124-a565-338214f77e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "cosmos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
